{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edf03bb4-0df9-44fb-b17e-69ee447cf912",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS default.Logs (\n",
    "  id BIGINT GENERATED ALWAYS AS IDENTITY (START WITH 1 INCREMENT BY 1) NOT NULL,\n",
    "  tabela STRING,\n",
    "  camada STRING,\n",
    "  status STRING,\n",
    "  origem STRING,\n",
    "  destino STRING,\n",
    "  temp_inicial TIMESTAMP,\n",
    "  temp_final TIMESTAMP,\n",
    "  linhas_origem INT,\n",
    "  linhas_processadas INT\n",
    ");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7aea4bd-1e51-4264-a6f8-63d441d5e782",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def addlog(log_tabela, log_camada, log_origem, log_destino, log_status, atualizacao):\n",
    "    '''\n",
    "    Log de processamento\n",
    "    - Mapeia o processamento da arquiteura medalhao\n",
    "    - Adiciona o log de processamento\n",
    "    - Atualiza o log de processamento\n",
    "    '''\n",
    "    if log_status is None:\n",
    "        log_status = 'null'\n",
    "\n",
    "    log_tempo = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    log_linhas_origem = spark.read.table(f\"{log_origem}.{log_tabela}\").count()\n",
    "    try:\n",
    "        log_linhas_destino = spark.read.table(f\"{log_destino}.{log_tabela}\").count()\n",
    "    except:\n",
    "        log_linhas_destino = 'null'\n",
    "\n",
    "    if atualizacao == 0:\n",
    "        query = f\"\"\"\n",
    "            INSERT INTO default.logs \n",
    "            (tabela, camada, status, origem, destino, temp_inicial, temp_final, linhas_origem, linhas_processadas) \n",
    "            VALUES ('{log_tabela}', '{log_camada}', 'Em processamento','{log_origem}', '{log_destino}', '{log_tempo}', null, {log_linhas_origem}, {log_linhas_destino})\n",
    "        \"\"\"\n",
    "        spark.sql(query)\n",
    "    \n",
    "    elif atualizacao == 1:\n",
    "        \n",
    "        query = f\"\"\"\n",
    "            UPDATE default.logs \n",
    "            SET temp_final = '{log_tempo}',\n",
    "                linhas_processadas = {log_linhas_destino},\n",
    "                status = '{log_status}'\n",
    "             WHERE id = (select id from default.logs where status = 'Em processamento' order by temp_inicial desc limit 1)\n",
    " \n",
    "        \"\"\"\n",
    "        spark.sql(query)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5683606963516749,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "LogProcessamento",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1f08f98-1db8-4596-9b42-2486abc2c5b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üìö Conceitos Fundamentais no Spark e Databricks\n",
    "\n",
    "## üîç 1. Quality Checks\n",
    "### ‚úÖ O que √©?\n",
    "Quality Checks s√£o **valida√ß√µes aplicadas aos dados para garantir sua integridade e qualidade**. Isso √© feito antes de usar os dados para an√°lise ou grava√ß√£o em um banco de dados para evitar inconsist√™ncias.\n",
    "\n",
    "### üìå Por que √© importante?\n",
    "- Garante que os dados estejam **corretos**, **consistentes** e **completos**.\n",
    "- Evita erros em an√°lises e processos posteriores.\n",
    "\n",
    "---\n",
    "\n",
    "## üîÄ 2. Deduplicate\n",
    "### ‚úÖ O que √©?\n",
    "Deduplicate √© o processo de **remover registros duplicados de um DataFrame ou tabela**. √â √∫til para evitar redund√¢ncias e garantir que cada linha seja √∫nica.\n",
    "\n",
    "### üìå Por que √© importante?\n",
    "- Mant√©m a **qualidade e integridade dos dados**.\n",
    "- Evita contagens erradas e c√°lculos incorretos.\n",
    "\n",
    "---\n",
    "\n",
    "## üìÖ 3. Window Function\n",
    "### ‚úÖ O que √©?\n",
    "As Window Functions permitem realizar **c√°lculos em grupos de linhas relacionadas**, mantendo cada linha original sem agrupar como o `groupBy()` faria.\n",
    "\n",
    "### üìå Por que √© importante?\n",
    "- Permite c√°lculos complexos como **ranking, m√©dias m√≥veis e agrega√ß√µes parciais**.\n",
    "- Facilita opera√ß√µes que exigem contexto sobre outras linhas sem perder os dados originais.\n",
    "\n",
    "---\n",
    "\n",
    "## üìù 4. Apply Schema\n",
    "### ‚úÖ O que √©?\n",
    "O **Apply Schema** √© o processo de definir explicitamente um esquema (`schema`) para um DataFrame, especificando **nomes de colunas e tipos de dados**.\n",
    "\n",
    "### üìå Por que √© importante?\n",
    "- **Garante consist√™ncia** no formato dos dados.\n",
    "- Evita erros de infer√™ncia autom√°tica do Spark que podem ocorrer com dados sujos ou mal formatados.\n",
    "\n",
    "---\n",
    "\n",
    "## üìÇ 5. Opera√ß√µes: MERGE, APPEND e OVERWRITE\n",
    "\n",
    "### üîÑ MERGE (Upsert)\n",
    "#### ‚úÖ O que √©?\n",
    "Combina dados de uma fonte com uma tabela existente, **atualizando registros correspondentes e inserindo novos registros**.\n",
    "\n",
    "#### üìå Uso Ideal:\n",
    "- Dados mestres que precisam ser atualizados periodicamente.\n",
    "\n",
    "---\n",
    "\n",
    "### üì• APPEND\n",
    "#### ‚úÖ O que √©?\n",
    "Adiciona **novos registros** a uma tabela sem modificar os existentes.\n",
    "\n",
    "#### üìå Uso Ideal:\n",
    "- Adicionar dados incrementais (logs, eventos, transa√ß√µes, etc.).\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ôªÔ∏è OVERWRITE\n",
    "#### ‚úÖ O que √©?\n",
    "**Substitui completamente** os dados de uma tabela, apagando o conte√∫do anterior.\n",
    "\n",
    "#### üìå Uso Ideal:\n",
    "- Atualiza√ß√µes completas ou substitui√ß√£o de dados desatualizados.\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Resumo das Opera√ß√µes\n",
    "\n",
    "| Opera√ß√£o  | Descri√ß√£o                                                      | Uso Ideal                                    |\n",
    "|-----------|----------------------------------------------------------------|--------------------------------------------|\n",
    "| **MERGE** | Atualiza registros existentes e insere novos dados.           | Dados mestres que precisam de atualiza√ß√µes. |\n",
    "| **APPEND**| Insere novos registros sem modificar os existentes.           | Dados incrementais (logs, eventos, etc.).  |\n",
    "| **OVERWRITE**| Substitui completamente os dados de uma tabela.            | Relat√≥rios completos ou recria√ß√£o de tabelas. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0343d283-fccd-44cc-b500-ed53d179eaf6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Bibliotecas"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import DataFrame, window\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.types import (StructType, StructField,\n",
    "        IntegerType, StringType, DoubleType, DecimalType, TimestampType, ShortType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "590efc18-8d2d-43fc-bb07-c6140a620cc9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Valida√ß√£o"
    }
   },
   "outputs": [],
   "source": [
    "def _validate_schema(df: DataFrame, expected_schema: StructType) -> bool:\n",
    "    \"\"\"\n",
    "    Valida se o schema do DataFrame corresponde ao schema esperado.\n",
    "\n",
    "    Par√¢metros:\n",
    "        df (DataFrame): O DataFrame a ser validado.\n",
    "        expected_schema (StructType): O schema esperado.\n",
    "\n",
    "    Retorna:\n",
    "        bool: True se o schema corresponder, False caso contr√°rio.\n",
    "    \"\"\"\n",
    "    actual_schema = df.schema\n",
    "\n",
    "    # Verifica se o n√∫mero de campos corresponde\n",
    "    if len(expected_schema.fields) != len(actual_schema.fields):\n",
    "        return False\n",
    "\n",
    "    # Verifica cada campo e tipo de dado\n",
    "    for i, field in enumerate(actual_schema.fields):\n",
    "        expected_field = expected_schema.fields[i]\n",
    "        if field.name != expected_field.name or not isinstance(field.dataType, type(expected_field.dataType)):\n",
    "            return False\n",
    "\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80054808-111b-42e5-bb80-e3085bf6f432",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Upsert"
    }
   },
   "outputs": [],
   "source": [
    "def _upsert_silver_table(transformed_df: DataFrame, target_table: str, primary_keys: list, not_matched_by_source_action: str = None, not_matched_by_source_condition: str = None) -> None:\n",
    "    \"\"\"\n",
    "    Realiza o upsert (update e insert) na tabela Delta da camada prata,\n",
    "    suportando a evolu√ß√£o do esquema e construindo dinamicamente a condi√ß√£o de merge.\n",
    "\n",
    "    Par√¢metros:\n",
    "        transformed_df (DataFrame): DataFrame contendo os dados transformados para inser√ß√£o na camada prata.\n",
    "        target_table (str): Nome da tabela de destino.\n",
    "        primary_keys (list): Lista de chaves prim√°rias para o merge.\n",
    "        not_matched_by_source_action (str, opcional): A√ß√£o a ser tomada quando uma linha da tabela de destino n√£o tiver correspond√™ncia na tabela de origem. Pode ser \"DELETE\" ou \"UPDATE\".\n",
    "        not_matched_by_source_condition (str, opcional): Condi√ß√£o adicional para aplicar a a√ß√£o definida em not_matched_by_source_action. -- use t.column = s.column -- t -> target / s -> source\n",
    "    \"\"\"\n",
    "    spark.sql(\"USE CATALOG hive_metastore\")\n",
    "    spark.sql(\"USE DATABASE adventure_works_silver\")\n",
    "\n",
    "    if not spark.catalog.tableExists(target_table):\n",
    "        transformed_df.write.format(\"delta\").saveAsTable(target_table)\n",
    "        print(f\"Tabela {target_table} criada.\")\n",
    "        return\n",
    "\n",
    "    \n",
    "    merge_condition = \" AND \".join([f\"s.{key} = t.{key}\" for key in primary_keys])\n",
    "\n",
    "    delta_table = DeltaTable.forName(spark, target_table) # DeltaTable.forName -> Carrega uma tabela Delta existente\n",
    "\n",
    "    merge_builder = delta_table.alias(\"t\").merge(\n",
    "        transformed_df.alias(\"s\"),\n",
    "        merge_condition\n",
    "    )\n",
    "\n",
    "    merge_builder = merge_builder.whenMatchedUpdateAll()\n",
    "\n",
    "    merge_builder = merge_builder.whenNotMatchedInsertAll() # -> Adicionar a cl√°usula WHEN NOT MATCHED (inserir novos registros)\n",
    "\n",
    "  # Se o par√¢metro not_matched_by_source_action for \"DELETE\", adicionar a l√≥gica para deletar linhas\n",
    "    if not_matched_by_source_action and not_matched_by_source_action.upper() == \"DELETE\":\n",
    "        # Obter as chaves das linhas na tabela de destino que n√£o t√™m correspond√™ncia na tabela de origem\n",
    "        unmatched_rows = delta_table.toDF().alias(\"t\").join(\n",
    "            transformed_df.alias(\"s\"),\n",
    "            on=[F.col(f\"t.{key}\") == F.col(f\"s.{key}\") for key in primary_keys],\n",
    "            how=\"left_anti\"\n",
    "        )\n",
    "\n",
    "        # Aplicar a condi√ß√£o adicional de exclus√£o, se fornecida\n",
    "        if not_matched_by_source_condition:\n",
    "            unmatched_rows = unmatched_rows.filter(not_matched_by_source_condition)\n",
    "\n",
    "        # Executar a exclus√£o das linhas n√£o correspondentes\n",
    "        delta_table.alias(\"t\").merge(\n",
    "            unmatched_rows.alias(\"s\"),\n",
    "            merge_condition\n",
    "        ).whenMatchedDelete().execute()\n",
    "\n",
    "    # Executar o merge\n",
    "    merge_builder.execute()\n",
    "    \n",
    "    print(\"Upsert executado com sucesso.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "DeltaFunctions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

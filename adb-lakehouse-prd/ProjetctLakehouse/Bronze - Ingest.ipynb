{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30d3c252-ac3d-4941-b0a1-c6c1074064bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **Ingestão de Dados para o Stage Bronze no Databricks**\n",
    "\n",
    "### **Descrição do Processo**  \n",
    "Este processo tem como objetivo realizar a ingestão de dados no **stage Bronze** da arquitetura Medallion. Ele lê dados em formato Parquet de um **Data Lake**, adiciona uma coluna de **timestamp** e salva as informações em um banco de dados Delta.\n",
    "\n",
    "---\n",
    "\n",
    "### **Origem do Processo**  \n",
    "Os dados são lidos a partir de arquivos **Parquet** localizados no **Data Lake**, processados com a adição de um **timestamp** e armazenados em uma tabela **Delta** no banco de dados especificado.\n",
    "\n",
    "---\n",
    "\n",
    "### **Etapas do Processamento**\n",
    "\n",
    "\n",
    "1. **`CMD 2 - Execução de Script de Parâmetros`**  \n",
    "   O comando `%run ./Parametros` executa o script de parâmetros, que carrega a lista `parameters` contendo as configurações de leitura e ingestão de dados (como `read_path`, `table_name` e `database_name`).\n",
    "\n",
    "2. **`CMD 3 - Importação de Funções do PySpark`**  \n",
    "   As funções necessárias são importadas do módulo `pyspark.sql.functions`:  \n",
    "   - `current_timestamp()`: Para adicionar a coluna de **timestamp** com a data e hora atuais.\n",
    "   - `col()`: Utilizada para referenciar colunas dentro de expressões (não utilizada diretamente neste código, mas é uma função útil para manipulação de colunas).\n",
    "\n",
    "\n",
    "3. **`CMD 4 - Comando SQL para Criação de Banco de Dados`**  \n",
    "   - Este comando SQL, comentado, seria responsável pela criação do banco de dados **`adventure_works_bronze`** se ele não existir. A localização do banco de dados seria no **Data Lake**.\n",
    "   - O código está comentado para não ser executado automaticamente.\n",
    "\n",
    "4. **`CMD 5 - Definição da Função `ingest_data_bronze`**  \n",
    "   A função `ingest_data_bronze` é responsável pela ingestão de dados do **stage Bronze**:\n",
    "   - **Parâmetros**:\n",
    "     - `read_path`: Caminho do arquivo Parquet no Data Lake.\n",
    "     - `table_name`: Nome da tabela que será criada no banco de dados Delta.\n",
    "     - `database_name`: Nome do banco de dados onde a tabela será salva.\n",
    "   - **Processamento**:\n",
    "     - Leitura do arquivo Parquet.\n",
    "     - Adição da coluna `timestamp_bronze` com o timestamp atual.\n",
    "     - Salvamento no formato **Delta**, sobrescrevendo a tabela existente, caso necessário.\n",
    "     - Em caso de erro, uma mensagem indicando a falha no processamento é impressa.\n",
    "\n",
    "5. **`CMD 6 - Execução da Função para Todos os Parâmetros**  \n",
    "   O código itera sobre a lista `parameters`, chamando a função `ingest_data_bronze` para cada conjunto de parâmetros definidos.  \n",
    "   Após o processamento de todos os dados, a mensagem \"Processamento finalizado\" é exibida.\n",
    "\n",
    "---\n",
    "\n",
    "### **Destino do Processo**  \n",
    "Após a execução, as tabelas **Delta** são salvas no banco de dados especificado, na camada **Bronze**, tornando os dados acessíveis para processamento posterior.\n",
    "\n",
    "---\n",
    "\n",
    "### **Observações Importantes**\n",
    "- A função **`ingest_data_bronze`** é robusta e pode lidar com vários conjuntos de dados de forma flexível, de acordo com os parâmetros fornecidos.\n",
    "- A tabela Delta é sobrescrita caso já exista, utilizando o modo `overwrite`.\n",
    "- A adição da coluna `timestamp_bronze` permite rastrear o momento exato da ingestão dos dados.\n",
    "- O processo pode ser repetido sempre que necessário, desde que o caminho de leitura e os parâmetros estejam corretamente configurados.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ccb76b8-4a3d-4e0f-8b18-d8df78697d58",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Parametros"
    }
   },
   "outputs": [],
   "source": [
    "%run ./Config/Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52cb0370-c80d-4808-b0b9-7d073446f582",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "log"
    }
   },
   "outputs": [],
   "source": [
    "%run ./Config/LogProcessamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fd294ee-f05d-4fe1-8fab-a1df9d43000f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Bibliotecas"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, col\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "dbutils.widgets.text('Tabela', '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "588b0d4b-a46c-4801-9d09-6ebdb0f757ec",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Database"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "USE CATALOG hive_metastore;\n",
    "\n",
    "CREATE DATABASE IF NOT EXISTS adventure_works_bronze\n",
    "LOCATION '/mnt/adlsstoragemasterprd/landing-zone/MedallionArchitecture/bronze/adventure_works_bronze';\n",
    "\n",
    "CREATE DATABASE IF NOT EXISTS adventure_works_silver\n",
    "LOCATION '/mnt/adlsstoragemasterprd/landing-zone/MedallionArchitecture/silver/adventure_works_silver';\n",
    "\n",
    "CREATE DATABASE IF NOT EXISTS adventure_works_gold\n",
    "LOCATION '/mnt/adlsstoragemasterprd/landing-zone/MedallionArchitecture/gold/adventure_works_gold';\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cc40fa5-257a-42b8-a40e-99dedfd7f880",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Ingest Bronze"
    }
   },
   "outputs": [],
   "source": [
    "def ingest_data_bronze(read_path, table_name, database_name):\n",
    "    \"\"\"\n",
    "    Ingestão de dados para o stage bronze no formato bruto\n",
    "        - Ler parquet do datalake\n",
    "        - Adicionar coluna timestamp_bronze\n",
    "        - Salvar em delta no banco de dados\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = spark.read.parquet(read_path)\n",
    "        df.write.mode('overwrite').option('overwriteSchema', True).format('delta').saveAsTable(f\"{database_name}.{table_name}\")\n",
    "        print(f\"✅ tabela {table_name} Processada com sucesso\")\n",
    "    except:\n",
    "        print(f\"❌ tabela {table_name} Falhou\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b81ee378-2b63-4afb-a6a0-a290e9ee7fab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tabela = dbutils.widgets.get('Tabela')\n",
    "log_data = {\n",
    "    \"log_tabela\": tabela,\n",
    "    \"log_camada\": \"Bronze\",\n",
    "    \"log_origem\": \"adventure_works_bronze\",\n",
    "    \"log_destino\": \"adventure_works_bronze\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ac6ae87-fd24-4c9d-8e95-f12ad248d41d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Processa tabela individual"
    }
   },
   "outputs": [],
   "source": [
    "tabela = dbutils.widgets.get('Tabela')\n",
    "\n",
    "if tabela != None and tabela != \"\":\n",
    "    for i in parameters:\n",
    "        if i[\"table_name\"] == tabela:\n",
    "            try:\n",
    "                addlog(**log_data, log_status='Sucesso',atualizacao=0)\n",
    "                ingest_data_bronze(i['read_path'], i['table_name'], i['database_name'])\n",
    "                addlog(**log_data, log_status='Sucesso',atualizacao=1)\n",
    "            except:\n",
    "                addlog(**log_data, log_status='Falha',atualizacao=1)\n",
    "    dbutils.notebook.exit(f\"✅ tabela {tabela} Processada com sucess\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0018348-2c0a-4ff9-9194-68d4e7f1cce3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Processamento Tabela"
    }
   },
   "outputs": [],
   "source": [
    "with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "    executor.map(lambda p: ingest_data_bronze(p['read_path'], p['table_name'], p['database_name']), parameters)\n",
    "\n",
    "print(\"Processamento finalizado\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7560131807243227,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Bronze - Ingest",
   "widgets": {
    "Tabela": {
     "currentValue": "Person_Password",
     "nuid": "89a8fc88-8cff-4159-be7b-9654a4b994f3",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "Tabela",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "Tabela",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

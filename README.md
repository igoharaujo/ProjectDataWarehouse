# Projeto Data Lakehouse

## ğŸ“Œ VisÃ£o Geral
Esse Projeto tem como objetivo criar um pipeline completo . A ingestÃ£o de dados ocorre a partir de um banco **OLTP**, transformando os dados em **Parquet** e armazenando-os em um **Data Lake**.

A arquitetura final seguirÃ¡ o modelo **Estrela**, mas essa etapa ainda nÃ£o foi implementada.

## ğŸ—ï¸ Arquitetura do Projeto
### Tecnologias Utilizadas
- **Azure Data Factory (ADF)** â†’ IngestÃ£o e orquestraÃ§Ã£o do ETL
- **Azure Data Lake** â†’ Armazenamento dos arquivos Parquet
- **Azure Databricks** â†’ Processamento e transformaÃ§Ã£o dos dados
- **Delta Lake** â†’ Formato otimizado para armazenamento e processamento
- **PySpark** â†’ ManipulaÃ§Ã£o e transformaÃ§Ã£o dos dados

## ğŸ”„ Processo de IngestÃ£o
A ingestÃ£o dos dados segue os seguintes passos:

1. **SeleÃ§Ã£o das tabelas para ingestÃ£o**
   - A lista de tabelas a serem processadas estÃ¡ armazenada na tabela `control_ingestion` no banco OLTP.
   - Consulta SQL utilizada:
   ```sql
   SELECT  [ID], [active], [source_schema], [source_table], [query], [folder], [file_name]
   FROM [dbo].[control_ingestion]
   WHERE active = 1;
   ```
   - A coluna `active` deve ser definida como `1` para indicar que a tabela deve ser atualizada.

2. **ExecuÃ§Ã£o no ADF**
   - Um **Lookup** consulta a `control_ingestion`.
   - Um loop **ForEach** executa duas atividades:
     - Envia a tabela para o **Data Lake** em formato **Parquet**.
     - Chama um **notebook no Databricks** para processar os dados (**bronze ingest**).

## ğŸ¥‰ Camada Bronze no Databricks

### ğŸ“Œ Objetivo
A camada **Bronze** tem como funÃ§Ã£o armazenar os dados brutos ingeridos no Data Lake e convertÃª-los para o formato **Delta Lake**.

### ğŸ”§ ConfiguraÃ§Ã£o Inicial
O **Databricks** realiza um **mount** para acessar os arquivos armazenados no **Data Lake**.

Comando para criaÃ§Ã£o do banco de dados (executado apenas uma vez):
```sql
--CREATE DATABASE IF NOT EXISTS adventure_works_bronze
--LOCATION '/mnt/adlsstorageworksprd/landing-zone/MedallionArchitecture/Bronze/adventure_works_bronze'
```

### ğŸ› ï¸ Processo de IngestÃ£o na Camada Bronze
1. **ImportaÃ§Ã£o de ParÃ¢metros**
   ```python
   %run ./Parametros
   ```
2. **Leitura dos Arquivos Parquet** e adiÃ§Ã£o de **timestamp**
3. **ConversÃ£o para Delta Lake** e gravaÃ§Ã£o no banco **Bronze**

CÃ³digo principal:
```python
from pyspark.sql.functions import current_timestamp

def ingest_data_bronze(read_path, table_name, database_name):
    try:
        df = spark.read.parquet(read_path)
        df = df.withColumn("timestamp_bronze", current_timestamp())
        df.write.format("delta").mode("overwrite").saveAsTable(f"{database_name}.{table_name}")
        print(f"Tabela {table_name} carregada com sucesso!")
    except Exception as e:
        print(f"Erro ao carregar {table_name}: {str(e)}")
```

4. **ExecuÃ§Ã£o para mÃºltiplas tabelas**
```python
for param in parameters:
    ingest_data_bronze(param["read_path"], param["table_name"], param["database_name"])
print("Processamento finalizado")
```

## ğŸ“Œ ConsideraÃ§Ãµes Finais
- A tabela Delta Ã© sobrescrita caso jÃ¡ exista (`overwrite`).
- A coluna `timestamp_bronze` ajuda a rastrear o momento da ingestÃ£o.
- A estrutura permite processamento flexÃ­vel e escalÃ¡vel.

---

ğŸ“ **PrÃ³ximos Passos:** ImplementaÃ§Ã£o da camada **Silver** e posteriormente **Gold**, seguindo a arquitetura Medallion.


# Projeto Data Warehouse

## üìå Vis√£o Geral
O **Projeto Data Warehouse** tem como objetivo criar um pipeline de ETL completo utilizando a abordagem **Full Load**. A ingest√£o de dados ocorre a partir de um banco **OLTP**, transformando os dados em **Parquet** e armazenando-os em um **Data Lake**.

A arquitetura final seguir√° o modelo **Estrela**, mas essa etapa ainda n√£o foi implementada.

## üèóÔ∏è Arquitetura do Projeto
### Tecnologias Utilizadas
- **Azure Data Factory (ADF)** ‚Üí Ingest√£o e orquestra√ß√£o do ETL
- **Azure Data Lake** ‚Üí Armazenamento dos arquivos Parquet
- **Azure Databricks** ‚Üí Processamento e transforma√ß√£o dos dados
- **Delta Lake** ‚Üí Formato otimizado para armazenamento e processamento
- **PySpark** ‚Üí Manipula√ß√£o e transforma√ß√£o dos dados

## üîÑ Processo de Ingest√£o
A ingest√£o dos dados segue os seguintes passos:

1. **Sele√ß√£o das tabelas para ingest√£o**
   - A lista de tabelas a serem processadas est√° armazenada na tabela `control_ingestion` no banco OLTP.
   - Consulta SQL utilizada:
   ```sql
   SELECT  [ID], [active], [source_schema], [source_table], [query], [folder], [file_name]
   FROM [dbo].[control_ingestion]
   WHERE active = 1;
   ```
   - A coluna `active` deve ser definida como `1` para indicar que a tabela deve ser atualizada.

2. **Execu√ß√£o no ADF**
   - Um **Lookup** consulta a `control_ingestion`.
   - Um loop **ForEach** executa duas atividades:
     - Envia a tabela para o **Data Lake** em formato **Parquet**.
     - Chama um **notebook no Databricks** para processar os dados (**bronze ingest**).

## ü•â Camada Bronze no Databricks

### üìå Objetivo
A camada **Bronze** tem como fun√ß√£o armazenar os dados brutos ingeridos no Data Lake e convert√™-los para o formato **Delta Lake**.

### üîß Configura√ß√£o Inicial
O **Databricks** realiza um **mount** para acessar os arquivos armazenados no **Data Lake**.

Comando para cria√ß√£o do banco de dados (executado apenas uma vez):
```sql
--CREATE DATABASE IF NOT EXISTS adventure_works_bronze
--LOCATION '/mnt/adlsstorageworksprd/landing-zone/MedallionArchitecture/Bronze/adventure_works_bronze'
```

### üõ†Ô∏è Processo de Ingest√£o na Camada Bronze
1. **Importa√ß√£o de Par√¢metros**
   ```python
   %run ./Parametros
   ```
2. **Leitura dos Arquivos Parquet** e adi√ß√£o de **timestamp**
3. **Convers√£o para Delta Lake** e grava√ß√£o no banco **Bronze**

C√≥digo principal:
```python
from pyspark.sql.functions import current_timestamp

def ingest_data_bronze(read_path, table_name, database_name):
    try:
        df = spark.read.parquet(read_path)
        df = df.withColumn("timestamp_bronze", current_timestamp())
        df.write.format("delta").mode("overwrite").saveAsTable(f"{database_name}.{table_name}")
        print(f"Tabela {table_name} carregada com sucesso!")
    except Exception as e:
        print(f"Erro ao carregar {table_name}: {str(e)}")
```

4. **Execu√ß√£o para m√∫ltiplas tabelas**
```python
for param in parameters:
    ingest_data_bronze(param["read_path"], param["table_name"], param["database_name"])
print("Processamento finalizado")
```

## üìå Considera√ß√µes Finais
- A tabela Delta √© sobrescrita caso j√° exista (`overwrite`).
- A coluna `timestamp_bronze` ajuda a rastrear o momento da ingest√£o.
- A estrutura permite processamento flex√≠vel e escal√°vel.

---

üìù **Pr√≥ximos Passos:** Implementa√ß√£o da camada **Silver** e posteriormente **Gold**, seguindo a arquitetura Medallion.

